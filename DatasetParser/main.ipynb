{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b436017d",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "This file contains the code for preprocess the raw data, downloaded from the GDC portal, into one large data frame that then gets saved into a large, about 188mb '.csv' file. Follow the instructions in the './Dateset/Settings/README.md' file to dowload the dataset. The dataset is also availible in te repository it self. Due to a bug in the GDC commandline tool, downloading became a long and tedious process, and so we decided to upload the dataset into the github repository.\n",
    "\n",
    "The dataset contains two types of data firstly the clinical data that takes the form of '.xml' files, these files contain important information like: isease_code, histological_type, vital_status and more. The second file that has a '.tvc' extention contains the RNA sequencing data of the case the file uses GENCODE v36 and has 60665 total genes.\n",
    "\n",
    "The added 'Dataset/Settings/gdc_manifest.2025-05-22.214548.txt' file is used to merge all processed files from each client into their own folder. Some clients have multiple RNA files, this is something to look out for. (Like for case '0a45f302-5748-48f3-9dc9-66c01843a68e')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa7d02e",
   "metadata": {},
   "source": [
    "#### 1. Defining global information\n",
    "\n",
    "Information like paths, gene-model type and minimal clinical columns are stored in variables for use throughout the code.\n",
    "Type anotations are used fore better clearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# NOTE: Make sure you run the project from the DatesetParser folder\n",
    "\n",
    "# Path to the dataset folder\n",
    "basePath: str = './Dataset/' \n",
    "# Path to settings folder\n",
    "settingsPath: str = basePath + 'Settings/'\n",
    "# Path to GDC data files\n",
    "inputPath: str = basePath + 'OriginalFiles/'\n",
    "# Path to the parsed files\n",
    "outputPath: str = basePath + 'ProcessedFiles/'\n",
    "# Path to the metadata file\n",
    "metadataPath: str = settingsPath + 'metadata.cart.2025-05-22.json'\n",
    "\n",
    "# Gene-model:\n",
    "geneModel: str = \"# gene-model: GENCODE v36\\n\"\n",
    "\n",
    "# The clinical must include the relevant columns\n",
    "columnsToKeep: set [str] = set(['histological_type', 'icd_o_3_histology'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e1e2b",
   "metadata": {},
   "source": [
    "#### 2. Helper functions\n",
    "\n",
    "Simple generelized functions that the main code uses, like reading a json file into a dictionary or changing the file extention of a given path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swaps the file extension of a given path with a new extension\n",
    "# Example: updateFileExtension('file.txt', 'csv') -> 'file.csv'\n",
    "def updateFileExtension(path: str, newExtension: str) -> str:\n",
    "    # Splitting the path into segments\n",
    "    segments: list[str] = str.split(path, '.')\n",
    "    # Removing the last segment (the current extension)\n",
    "    segments: list[str] = segments[0:-1]\n",
    "    # Adding the new extension\n",
    "    segments.append(newExtension)\n",
    "    # Joining the remaining segments back together\n",
    "    return '.'.join(segments)\n",
    "\n",
    "# Reading the a given json file and parsing it into a dictionary\n",
    "def readJsonFile(path: str) -> dict:\n",
    "    import json\n",
    "    with open(path) as f:\n",
    "        dictionary: dict = json.load(f)\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa88749",
   "metadata": {},
   "source": [
    "#### 3. Processing gene data\n",
    "\n",
    "The gene data is processed in multiple steps. \n",
    "First the header of the file is verified, to make sure it uses te right gene vesion. Then the data is loaded into a dataframe and filterd on 'lncRNA' 'gene_type'. After this the data is flattend into a column for each 'tpm_unstranded' and 'unstranded' row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c3197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processGeneData(originalPath: str) -> tuple[pd.DataFrame | None, str]:\n",
    "    with open(originalPath) as file:\n",
    "        header: str = file.readline()\n",
    "\n",
    "        # Verifying if the first line has the correct gene model\n",
    "        if header == geneModel:\n",
    "            # Updating the header variable\n",
    "            header = file.readline()\n",
    "\n",
    "            # Reading the file into a pandas dataframe\n",
    "            dataframe: pd.DataFrame = pd.read_csv(originalPath, delimiter=\"\\t\", skiprows=1)\n",
    "\n",
    "            # Filtering the dataframe to only include lncRNA\n",
    "            dataframe = dataframe[dataframe['gene_type'] == 'lncRNA']\n",
    "\n",
    "            # Creating the columns for unstranded and tpm values\n",
    "            dataframe['unstranded_col'] = dataframe['gene_name'] + '_unstranded'\n",
    "            dataframe['tpm_col'] = dataframe['gene_name'] + '_tpm_unstranded'\n",
    "\n",
    "            # Build a dictionary with new column names and their values\n",
    "            unstranded_vals: dict = dict(zip(dataframe['unstranded_col'], dataframe['unstranded']))\n",
    "            tpm_vals: dict = dict(zip(dataframe['tpm_col'], dataframe['tpm_unstranded']))\n",
    "\n",
    "            # Merge the two into one row using a dictionary\n",
    "            combined: dict = {**unstranded_vals, **tpm_vals}\n",
    "\n",
    "            # Create a new single-row DataFrame from the combined dict\n",
    "            result_df: pd.DataFrame = pd.DataFrame([combined])\n",
    "\n",
    "        return result_df, header    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f67062",
   "metadata": {},
   "source": [
    "#### 4. Processing clinical data\n",
    "\n",
    "The clinical data is processed into multiple steps. First the xml file is loaded into a dataframe. Then the data is flattend, due to the nested nature of the xml data. Lastly the data is checked for required headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processClinicalData(originalPath: str) -> pd.DataFrame | None:\n",
    "    # Loading the clinical data from the original file\n",
    "    dataFrame: pd.DataFrame = pd.read_xml(originalPath)\n",
    "\n",
    "    # Flattening the XML data\n",
    "    firstRow = dataFrame.loc[0]\n",
    "    for item in dataFrame.columns:\n",
    "        # Checking if the column is empty and removing it\n",
    "        if firstRow[item] is None or firstRow[item] == '':\n",
    "            dataFrame.loc[0, item] = dataFrame.loc[1, item]  # Copying the value from the second row\n",
    "\n",
    "    # Checking if the columns exist in the dataframe\n",
    "    if not columnsToKeep.issubset(dataFrame.columns):\n",
    "        print(\"Some columns are missing in the clinical data. Please check the file.\")\n",
    "        return None\n",
    "    \n",
    "    # The xml file is not parsed flat but in two rows, so we need to flatten it\n",
    "    dataFrame.drop(index=1, inplace=True)\n",
    "  \n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9adf95",
   "metadata": {},
   "source": [
    "#### 5. Mering data (Using Metadata)\n",
    "\n",
    "The function starts with defining two dataframes for the two separate dataset. Then the metadata is loaded in. Every element in the metadata file is evaluated and processed. First the 'case_id' is retrieved, then extra information to form the input and ouput urls are loaded. A simple log is pinted to the screen to indicate and visualise progress. Next the file wil be checked for existence. After the file extension is checked to split the parsing of each type in their respectable functions. The result is verfied to be non NONE and non empty. This result is then concatinated to its respectable dataframe an optionaly saved to its case output folder. Lasty the function merges all the data into one large dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeCaseData(metadataPath: str, inputPath: str, outputPath: str, storeSubfiles: bool = True) -> pd.DataFrame:\n",
    "    import os\n",
    "\n",
    "    # Initializing the two main data frames with the case_id column\n",
    "    dataFrameColumns: list[str] = ['case_id']\n",
    "    clinicalDataFrame: pd.DataFrame = pd.DataFrame(columns=dataFrameColumns)\n",
    "    geneDataFrame: pd.DataFrame = pd.DataFrame(columns=dataFrameColumns)\n",
    "\n",
    "    # Initializing the file count\n",
    "    fileCount: int = 0\n",
    "\n",
    "    # Reading the metadata file, with contains information about merging case files\n",
    "    metaData: dict = readJsonFile(metadataPath)\n",
    "    for file in metaData:\n",
    "        # Creating the output folder for the case\n",
    "        caseId = file['associated_entities'][0]['case_id']\n",
    "\n",
    "        # Retrieving the file name and folder name\n",
    "        fileName: str = file['file_name']\n",
    "        folderName: str = file['file_id']\n",
    "        fileFormat: str = file['data_format']\n",
    "\n",
    "        # Increasing the file count and printing an update to indicate progress\n",
    "        fileCount += 1\n",
    "        print(f\"Processing file {fileCount}/{len(metaData)}: {fileName} for case {caseId}\")\n",
    "\n",
    "        # Creating the storage and output paths\n",
    "        dataFile = inputPath + folderName + '/' + fileName\n",
    "        outputFile = updateFileExtension(outputPath + caseId + '/' + fileName, \"csv\")\n",
    "  \n",
    "        # Checking if the file exists (Data set may be corrupted or incomplete)\n",
    "        if not os.path.isfile(dataFile):\n",
    "            print(\"File not found: \" + dataFile)\n",
    "            continue\n",
    "\n",
    "         # Handling the different file types\n",
    "        dataFrame = None\n",
    "        if fileFormat == 'TSV':\n",
    "            (dataFrame, _) = processGeneData(dataFile) \n",
    "        else:  \n",
    "            dataFrame = processClinicalData(dataFile)\n",
    "\n",
    "        # Adding the data to the main dataframe and storing the file\n",
    "        if dataFrame is not None and not dataFrame.empty:\n",
    "            dataFrame['case_id'] = caseId\n",
    "         \n",
    "            # Adding the loaded data frame to the main data frame\n",
    "            if fileFormat == 'TSV':\n",
    "                geneDataFrame = pd.concat([geneDataFrame, dataFrame])\n",
    "            else:  \n",
    "                clinicalDataFrame = pd.concat([clinicalDataFrame, dataFrame])\n",
    "             \n",
    "            # Only storing the subfiles if the flag is set\n",
    "            if storeSubfiles:\n",
    "                dataFrame.to_csv(outputFile, index=False, header=True)\n",
    "\n",
    "    # Merging the clinical and gene data frames on the case_id column\n",
    "    return pd.merge(clinicalDataFrame, geneDataFrame, on=\"case_id\", how=\"inner\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421225d",
   "metadata": {},
   "source": [
    "#### 6. Run processing\n",
    "\n",
    "The main processing function is called where after the result is printed and stored in the 'merged_data.csv' output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2483ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the main processing function\n",
    "mainDataFrame: pd.DataFrame = mergeCaseData(metadataPath, inputPath, outputPath, True)\n",
    "\n",
    "# Displaying the main data frame for debugging purposes\n",
    "print(mainDataFrame)\n",
    "\n",
    "# Storing the main data frame to a file\n",
    "mainDataFrame.to_csv(outputPath + 'merged_data.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
